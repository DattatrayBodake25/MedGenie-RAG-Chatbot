{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kPxOPnNv79"
      },
      "source": [
        "# Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJvjH2y_OFr1"
      },
      "source": [
        "### 1. Installing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WJCMqy6WOIls"
      },
      "outputs": [],
      "source": [
        "# %pip install numpy==1.24.3 pandas==2.0.3\n",
        "# %pip install -qU langchain-text-splitters langchain-community langchain-google-genai langchain-huggingface langchain-pinecone langgraph transformers sentence-transformers pinecone-client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jZYuowaQUNd"
      },
      "source": [
        "### 2.Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_mBYOjZQQrok"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "from langgraph.graph import START, StateGraph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pinecone\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XakcovPPR76-"
      },
      "source": [
        "# Document Loding and Pre-processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HKVBqAjgSA1M"
      },
      "outputs": [],
      "source": [
        "# Load the dataset (use the correct path to your MedQuad dataset)\n",
        "dataset_path = \"medquad.csv\"\n",
        "loader = CSVLoader(file_path = dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "R6IuD6taShvf"
      },
      "outputs": [],
      "source": [
        "# Load the documents from the CSV\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a2id5zCZUgu6"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function for documents cleaning\n",
        "def preprocess_text(text):\n",
        "    # Step 1: Remove extra spaces and newlines\n",
        "    text = text.strip()\n",
        "    text = \" \".join(text.split())  # remove extra spaces between words\n",
        "\n",
        "    # Step 2: Remove non-informative content (e.g., footer, page numbers, unnecessary metadata)\n",
        "    text = re.sub(r'\\(.*\\)', '', text)  # Removes text inside parentheses (e.g., page numbers or notes)\n",
        "\n",
        "    # Step 3: Extract useful metadata (e.g., source, focus_area) and remove from text if needed\n",
        "    source_match = re.search(r'source:\\s*(.*)', text)\n",
        "    focus_area_match = re.search(r'focus_area:\\s*(.*)', text)\n",
        "    metadata = {\n",
        "        'source': source_match.group(1) if source_match else '',\n",
        "        'focus_area': focus_area_match.group(1) if focus_area_match else ''\n",
        "    }\n",
        "\n",
        "    # Step 4: Remove the extracted metadata from text\n",
        "    text = re.sub(r'source:.*', '', text)  # Remove the \"source\" part\n",
        "    text = re.sub(r'focus_area:.*', '', text)  # Remove the \"focus_area\" part\n",
        "\n",
        "    # Step 5: Remove punctuation (optional, depends on the task)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Step 6: Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text, metadata\n",
        "\n",
        "# Loop through each document and apply preprocessing\n",
        "for doc in documents:\n",
        "    cleaned_text, doc_metadata = preprocess_text(doc.page_content)\n",
        "    doc.page_content = cleaned_text\n",
        "    doc.metadata.update(doc_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4S0sqYMV4jW"
      },
      "source": [
        "# Splitting Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp9QGiADV7ND",
        "outputId": "6ba8fb6e-4a0b-4411-f5ee-95282daacafb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of split documents: 22372\n",
            "Sample split document:\n",
            "question what is  in glaucoma for still unknown reasons the fluid drains too slowly out of the eye as the fluid builds up the pressure inside the eye rises unless this pressure is controlled it may cause damage to the optic nerve and other parts of the eye and result in loss of vision openangle glaucoma the most common type of glaucoma is called openangle glaucoma in the normal eye the clear fluid leaves the anterior chamber at the open angle where the cornea and iris meet when fluid reaches the\n"
          ]
        }
      ],
      "source": [
        "# Initialize the RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "\n",
        "# Split the document into smaller chunks\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Preview the split documents\n",
        "print(f\"Number of split documents: {len(split_docs)}\")\n",
        "print(\"Sample split document:\")\n",
        "print(split_docs[0].page_content[:500])  # Show first 500 characters of the first chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XTXmbs0X1d3"
      },
      "source": [
        "# Creating Embeddings and Upsert in Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-s3oTmLgX553",
        "outputId": "acf81ef8-15b0-4491-b537-d5855f4f019c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 3 Document IDs: ['db21ba52-d381-49e0-b3d1-ebb306e23301', '9315c60e-a8d6-4288-b6b8-0c9db4efd464', 'c3fa64c2-3876-4da2-820f-29ad63e13c80']\n"
          ]
        }
      ],
      "source": [
        "# Initialize the embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Initialize Pinecone client with your API key\n",
        "pc = Pinecone(api_key=\"Paste-Your-Pinecone-API-Key-Here\")\n",
        "\n",
        "# Connect to the specific index\n",
        "index_name = \"medical-rag-chatbot\"\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Set up the VectorStore\n",
        "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n",
        "\n",
        "# Add the document chunks to the vector store\n",
        "document_ids = vector_store.add_documents(documents=split_docs)\n",
        "\n",
        "# Print the first 3 document IDs to verify\n",
        "print(f\"First 3 Document IDs: {document_ids[:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W6CdhjeeFXJ"
      },
      "source": [
        "# Querying and Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCGg5ZSEi8Dv",
        "outputId": "8db1cec1-4709-4f85-f19f-027236ac22c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved Documents for the query 'What is glaucoma and its treatment?':\n",
            "question what is  glaucoma  answer glaucoma is a group of diseases that can damage the eyes optic nerve it is a leading cause of blindness in the united states it usually happens when the fluid pressure inside the eyes slowly rises damaging the optic nerve often there are no symptoms at first withou\n",
            "to ways some patients are coping with glaucoma surgery laser surgery is another treatment for glaucoma during laser surgery a strong beam of light is focused on the part of the anterior chamber where the fluid leaves the eye this results in a series of small changes that makes it easier for fluid to\n",
            "question what is  in glaucoma for still unknown reasons the fluid drains too slowly out of the eye as the fluid builds up the pressure inside the eye rises unless this pressure is controlled it may cause damage to the optic nerve and other parts of the eye and result in loss of vision openangle glau\n"
          ]
        }
      ],
      "source": [
        "# Define the function for retrieval\n",
        "def retrieve(query: str, k: int = 3):  # Use 'k' for the number of results to return\n",
        "    # Perform similarity search to retrieve the most relevant documents\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=k)\n",
        "\n",
        "    # Return the retrieved documents\n",
        "    return retrieved_docs\n",
        "\n",
        "# Example user query\n",
        "query = \"What is glaucoma and its treatment?\"\n",
        "\n",
        "# Retrieve relevant documents for the query\n",
        "retrieved_docs = retrieve(query)\n",
        "\n",
        "# Display the retrieved documents\n",
        "print(f\"Retrieved Documents for the query '{query}':\")\n",
        "for doc in retrieved_docs:\n",
        "    print(doc.page_content[:300])  # Print the first 300 characters of each retrieved document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqXSWH7ytAhB"
      },
      "source": [
        "# LLM Integration for Generating Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjn4o_JKtBME",
        "outputId": "d9e1ed13-2185-4230-aeea-d7956470d534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Lymphocytic colitis is a type of microscopic colitis, an inflammation of the colon only visible under a microscope.  Symptoms may include chronic watery diarrhea, abdominal pain, cramping, bloating, weight loss, nausea, dehydration, and/or fecal incontinence. The underlying cause is unknown, but may involve autoimmune conditions, medications, infections, genetic factors, and/or bile acid malabsorption. Treatment depends on individual symptoms and may include medications, dietary changes, and rarely, surgery.\n"
          ]
        }
      ],
      "source": [
        "# Define the state of the application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# Initialize the Gemini API key for authentication\n",
        "gemini_api_key = \"Paste-Your-Gemini-API-Key-Here\"\n",
        "\n",
        "# Initialize the ChatGoogleGenerativeAI model with the Gemini API key\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=gemini_api_key, temperature=0)\n",
        "\n",
        "# Define the retrieval step\n",
        "def retrieve(state: State):\n",
        "    # Perform similarity search to retrieve relevant documents\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=5)\n",
        "\n",
        "    # If no documents found, add a custom message\n",
        "    if not retrieved_docs:\n",
        "        print(\"No relevant documents found. Please refine your query.\")\n",
        "\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "# Define the generation step\n",
        "def generate(state: State):\n",
        "    # Check if there are retrieved documents\n",
        "    if not state[\"context\"]:\n",
        "        return {\"answer\": \"Sorry, no relevant documents found. Please refine your query to be more specific to medical topics.\"}\n",
        "\n",
        "    # Extract the content of the retrieved documents\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "\n",
        "    # Define the improved prompt template\n",
        "    template = \"\"\"\n",
        "    You are an assistant trained to answer medical questions. Use the following pieces of context to directly answer the user's question.\n",
        "    If you don't know the answer or if the context does not seem relevant, respond with:\n",
        "    \"Sorry, I couldn't find relevant information. Please refine your query to be more related to medical topics.\"\n",
        "\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the prompt using the template\n",
        "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    # Format the prompt\n",
        "    prompt = custom_rag_prompt.format(question=state[\"question\"], context=docs_content)\n",
        "\n",
        "    # Generate the answer using the LLM\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Directly access the content of the response\n",
        "    answer_content = response.content.strip()  # Accessing 'content' directly\n",
        "\n",
        "    # Return the cleaned answer\n",
        "    return {\"answer\": answer_content}\n",
        "\n",
        "# Create a sequence of steps (retrieval -> generation)\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Test the application with a query\n",
        "result = graph.invoke({\"question\": \"What is the Lymphocytic Colitis?\"})\n",
        "\n",
        "# Print the generated answer\n",
        "print(f\"Answer: {result['answer']}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
